# -*- coding: utf-8 -*-
"""preprocessDefinition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yDQRtvzGMZz1UOVPgx4kMw1A80OsLZQC
"""

import tensorflow as tf

raw_dataset=tf.data.TFRecordDataset(['b/content/drive/MyDrive/Applied ML/Assignment 2/birds-vs-squirrels-train.tfrecords'])
feature_description={'image':tf.io.FixedLenFeature([],tf.string),
                     'label':tf.io.FixedLenFeature([],tf.int64)}

def parse_examples(serialized_examples):
    examples=tf.io.parse_example(serialized_examples,feature_description)
    targets=examples.pop('label')
    images=tf.image.resize_with_pad(tf.cast(tf.io.decode_jpeg(
        examples['image'],channels=3),tf.float32),299,299)
    return images, targets

#you can edit batch size and num_parallel calls below based on your architecture
dataset=raw_dataset.map(parse_examples,num_parallel_calls=16).batch(128)

dataset

# Calculate the dataset size manually
dataset_size = 0
for _ in dataset:
    dataset_size += 1

# Split the dataset into train and test sets
train_size = int(0.6 * dataset_size)
valid_size = int(0.2 * dataset_size)
test_size = dataset_size - train_size - valid_size

trainset = dataset.take(train_size)
validset = dataset.skip(train_size).take(valid_size)
testset = dataset.skip(train_size + valid_size)

# Print the number of samples in each set
print("Train set size:", train_size)
print("Validation set size:", valid_size)
print("Test set size:", test_size)

def preprocessWithAspectRatio(image,label): 
  resized_image=tf.image.resize_with_pad(image,299,299) 
  final_image=keras.applications.xception.preprocess_input(resized_image) 
  return final_image,label

"""# New Section"""