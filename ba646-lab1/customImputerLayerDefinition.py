# -*- coding: utf-8 -*-
"""customImputeLayerDefinition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P21x6ElTtbhdCVARbboICxEuwlzQ2Sqk
"""

from google.colab import drive 
drive.mount('/content/gdrive')

import tensorflow as tf
from tensorflow import keras

# Define a new class for custom imputation layer
class ImputerLayer(keras.layers.Layer):
    def adapt(self, data):
      non_nan_data = tf.boolean_mask(data, tf.logical_not(tf.math.is_nan(data)))
      self.mean = tf.reduce_mean(non_nan_data, axis=0)

    def call(self, inputs):
      nan_mask = tf.math.is_nan(inputs)
      return tf.where(nan_mask, tf.fill(tf.shape(inputs), self.mean), inputs)

# Define a method to parse serialized example protobufs
def parse_example(example_proto):
    feature_description = {
        'tickers': tf.io.FixedLenFeature([188], tf.float32),
        'weekday': tf.io.FixedLenFeature([], tf.int64),
        'hour': tf.io.FixedLenFeature([], tf.int64),
        'month': tf.io.FixedLenFeature([], tf.int64),
        'target': tf.io.FixedLenFeature([], tf.int64),
    }
    example = tf.io.parse_single_example(example_proto, feature_description)
    features = {k: example[k] for k in example.keys() if k != 'target'}
    target = example['target']
    return features, target

# Load the dataset from the tfrecords file and batch it
batch_size = 32
tfrecords_dataset = tf.data.TFRecordDataset("/content/gdrive/MyDrive/Applied ML/Assignment 1/MLAssignment1.tfrecord")
parsed_dataset = tfrecords_dataset.map(parse_example)
cached_dataset = parsed_dataset.cache()
batched_dataset = cached_dataset.batch(batch_size)

# Split the loaded dataset into training, validation, and testing datasets
train_size = int(0.7 * batch_size)
val_size = int(0.2 * batch_size)
test_size = batch_size - train_size - val_size
train_dataset = batched_dataset.take(train_size)
val_dataset = batched_dataset.skip(train_size).take(val_size)
test_dataset = batched_dataset.skip(train_size + val_size).take(test_size)

# Create a series of 4 keras inputs for the 4 labels in the instance dictionary
tickers_input = keras.Input(shape=(188,), dtype=tf.float32, name="tickers")
weekday_input = keras.Input(shape=(), dtype=tf.int64, name="weekday")
hour_input = keras.Input(shape=(), dtype=tf.int64, name="hour")
month_input = keras.Input(shape=(), dtype=tf.int64, name="month")

#occassionally the next set of commands will through an endpoint error, these commands are to remount the drive in that event. seems to fix the problem
!usermount -u drive
!google-drive-ocamlfuse drive

imputer_layer = ImputerLayer()
for batch in train_dataset:
    tickers = batch[0]['tickers']
    imputer_layer.adapt(tickers)

normalizer = preprocessing.Normalization()
for batch in train_dataset:
    tickers = batch[0]['tickers']
    imputed_tickers = imputer_layer(tickers)
    normalizer.adapt(imputed_tickers)

imputed_tickers = imputer_layer(tickers_input)
normalized_tickers = normalizer(imputed_tickers)
print(imputed_tickers)
print(normalized_tickers)