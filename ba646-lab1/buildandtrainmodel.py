# -*- coding: utf-8 -*-
"""buildAndTrainModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RyiLJMLTNWLWfUkROTS4HEVARK3qFZNg
"""

from google.colab import drive 
drive.mount('/content/gdrive')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, BatchNormalization
from tensorflow.keras.layers.experimental import preprocessing

# Define a new class for custom imputation layer
class ImputerLayer(keras.layers.Layer):
    def adapt(self, data):
      non_nan_data = tf.boolean_mask(data, tf.logical_not(tf.math.is_nan(data)))
      self.mean = tf.reduce_mean(non_nan_data, axis=0)

    def call(self, inputs):
      nan_mask = tf.math.is_nan(inputs)
      return tf.where(nan_mask, tf.fill(tf.shape(inputs), self.mean), inputs)

# Define a method to parse serialized example protobufs
def parse_example(example_proto):
    feature_description = {
        'tickers': tf.io.FixedLenFeature([188], tf.float32),
        'weekday': tf.io.FixedLenFeature([], tf.int64),
        'hour': tf.io.FixedLenFeature([], tf.int64),
        'month': tf.io.FixedLenFeature([], tf.int64),
        'target': tf.io.FixedLenFeature([], tf.int64),
    }
    example = tf.io.parse_single_example(example_proto, feature_description)
    features = {k: example[k] for k in example.keys() if k != 'target'}
    target = example['target']
    return features, target

# Load the dataset from the tfrecords file and batch it
batch_size = 32
tfrecords_dataset = tf.data.TFRecordDataset("/content/gdrive/MyDrive/Applied ML/Assignment 1/MLAssignment1.tfrecord")
parsed_dataset = tfrecords_dataset.map(parse_example)
cached_dataset = parsed_dataset.cache()
batched_dataset = cached_dataset.batch(batch_size)

# Define a method to parse serialized example protobufs
def parse_example(example_proto):
    feature_description = {
        'tickers': tf.io.FixedLenFeature([188], tf.float32),
        'weekday': tf.io.FixedLenFeature([], tf.int64),
        'hour': tf.io.FixedLenFeature([], tf.int64),
        'month': tf.io.FixedLenFeature([], tf.int64),
        'target': tf.io.FixedLenFeature([], tf.int64),
    }
    example = tf.io.parse_single_example(example_proto, feature_description)
    features = {k: example[k] for k in example.keys() if k != 'target'}
    target = example['target']
    return features, target

# Split the loaded dataset into training, validation, and testing datasets
train_size = int(0.7 * batch_size)
val_size = int(0.2 * batch_size)
test_size = batch_size - train_size - val_size
train_dataset = batched_dataset.take(train_size)
val_dataset = batched_dataset.skip(train_size).take(val_size)
test_dataset = batched_dataset.skip(train_size + val_size).take(test_size)

# Create a series of 4 keras inputs for the 4 labels in the instance dictionary
tickers_input = keras.Input(shape=(188,), dtype=tf.float32, name="tickers")
weekday_input = keras.Input(shape=(), dtype=tf.int64, name="weekday")
hour_input = keras.Input(shape=(), dtype=tf.int64, name="hour")
month_input = keras.Input(shape=(), dtype=tf.int64, name="month")

imputer_layer = ImputerLayer()
for batch in train_dataset:
    tickers = batch[0]['tickers']
    imputer_layer.adapt(tickers)

normalizer = preprocessing.Normalization()
for batch in train_dataset:
    tickers = batch[0]['tickers']
    imputed_tickers = imputer_layer(tickers)
    normalizer.adapt(imputed_tickers)

imputed_tickers = imputer_layer(tickers_input)
normalized_tickers = normalizer(imputed_tickers)

# Create a series of 4 keras inputs for the 4 labels in the instance dictionary
tickers_input = keras.Input(shape=(188,), dtype=tf.float32, name="tickers")
weekday_input = keras.Input(shape=(), dtype=tf.int64, name="weekday")
hour_input = keras.Input(shape=(), dtype=tf.int64, name="hour")
month_input = keras.Input(shape=(), dtype=tf.int64, name="month")

# Split the loaded dataset into training, validation, and testing datasets
train_size = int(0.7 * batch_size)
val_size = int(0.2 * batch_size)
test_size = batch_size - train_size - val_size
train_dataset = batched_dataset.take(train_size)
val_dataset = batched_dataset.skip(train_size).take(val_size)
test_dataset = batched_dataset.skip(train_size + val_size).take(test_size)

# Define input layers for the various features
num_tickers = 188
tickers_input = Input(shape=(1,), name='tickers_input')
weekday_input = Input(shape=(1,), name='weekday_input')
hour_input = Input(shape=(1,), name='hour_input')
month_input = Input(shape=(1,), name='month_input')

# Define embedding layers for the features
ticker_embedding = Embedding(input_dim=num_tickers, output_dim=16, name='ticker_embedding')(tickers_input)
weekday_embedding = Embedding(input_dim=7, output_dim=16, name='weekday_embedding')(weekday_input)
hour_embedding = Embedding(input_dim=24, output_dim=16, name='hour_embedding')(hour_input)
month_embedding = Embedding(input_dim=12, output_dim=16, name='month_embedding')(month_input)

# Flatten the embedding layers
ticker_flattened = Flatten()(ticker_embedding)
weekday_flattened = Flatten()(weekday_embedding)
hour_flattened = Flatten()(hour_embedding)
month_flattened = Flatten()(month_embedding)

# Normalize the tickers
tickers_normalized = BatchNormalization()(ticker_flattened)

# Stack the output of the embedding layers and the normalized tickers
stacked = Concatenate()([tickers_normalized, weekday_flattened, hour_flattened, month_flattened])
stacked = Dense(units=64, activation='relu')(stacked)

# Define the output layer
output_layer = Dense(units=1, activation='sigmoid')(stacked)

# Define the model
model = tf.keras.models.Model(inputs=[tickers_input, weekday_input, hour_input, month_input], outputs=output_layer)

class ImputerLayer(keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, inputs):
        mask = tf.math.logical_not(tf.math.is_finite(inputs))
        mask = tf.cast(mask, tf.float32)
        mean = tf.reduce_mean(tf.where(tf.math.is_finite(inputs), inputs, tf.zeros_like(inputs)), axis=0)
        inputs = tf.where(tf.math.is_finite(inputs), inputs, tf.zeros_like(inputs) + mean)
        return inputs

    def get_config(self):
        return super().get_config()

    @classmethod
    def from_config(cls, config):
        return cls(**config)

weekday_input = keras.layers.Input(shape=(1,), dtype='int64', name='weekday_input')
hour_input = keras.layers.Input(shape=(1,), dtype='int64', name='hour_input')
month_input = keras.layers.Input(shape=(1,), dtype='int64', name='month_input')

tickers_input = keras.Input(shape=(188,), dtype=tf.float32, name="tickers")
weekday_input = keras.Input(shape=(1,), dtype=tf.int64, name="weekday")
hour_input = keras.Input(shape=(1,), dtype=tf.int64, name="hour")
month_input = keras.Input(shape=(1,), dtype=tf.int64, name="month")
imputer_layer = ImputerLayer()
ticker_imputed = imputer_layer(tickers_input)

weekday_input_float = keras.layers.Lambda(lambda x: tf.cast(x, 'float32'))(weekday_input)
hour_input_float = keras.layers.Lambda(lambda x: tf.cast(x, 'float32'))(hour_input)
month_input_float = keras.layers.Lambda(lambda x: tf.cast(x, 'float32'))(month_input)
x = keras.layers.Concatenate()([ticker_imputed, weekday_input_float, hour_input_float, month_input_float])
x = keras.layers.Dense(300, activation='relu')(x)
x = keras.layers.Dropout(rate = 0.2)(x)
x = keras.layers.Dense(100, activation='relu')(x)
x = keras.layers.BatchNormalization()(x)
output = keras.layers.Dense(22, activation='softmax')(x)
model = keras.Model(inputs=[tickers_input, weekday_input, hour_input, month_input], outputs=output)

model.summary()

# Compile your model
model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

checkpoint_cb=keras.callbacks.ModelCheckpoint("my_keras_model.h5",save_best_only=True)
early_stopping_cb=keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)

# Fit your model to the training data
history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, batch_size=2, callbacks=[checkpoint_cb,early_stopping_cb])

!pip install pyyaml h5py  # Required to save models in HDF5 format

import os

model.save('mySavedModel')

# Evaluate your model on the test data
mse_test = test_loss, test_acc = model.evaluate(test_dataset)